The MapReduce programming model, pioneered by Google, is designed to process big data using a large number of machines. In a MapReduce program, data, stored as Key/Value pairs, is processed by a Map function. Mappers output a transformed set of Key/Value pairs, which are subsequently processed by a Reduce function.

MapReduce was designed to run in large server farms similar to machines that are deployed at Google's data centers and is proprietary. Hadoop is an open-source implementation of Google's MapReduce, but I will not explain it in detail here . Hadoop presents MapReduce as an analytics engine and, "under the hood," employs the Hadoop Distributed File System (HDFS). HDFS partitions input datasets into fixed-size chunks (blocks), distributing them on participating cluster nodes. Jobs can subsequently process HDFS blocks in parallel at distributed machines, thus exploiting the parallelism enabled by partitioning datasets. MapReduce breaks jobs into multiple tasks denoted as map and reduce tasks. All map tasks are encapsulated in the "map phase", and reduce tasks are encompassed in the "reduce phase". The map phase can have one or many map tasks, and the reduce phase can have zero or many reduce tasks.

Amazon's Elastic MapReduce is a PaaS implementation of Hadoop, designed for quick provisioning of Hadoop clusters and fast transfers to/from S3. In this part of the project, I can understand some of the key aspects of Elastic MapReduce by implementing the Map and Reduce funtions and running an Elastic MapReduce job flow.
